# torchop

A collection of some attention/convolution operators implemented using PyTorch.

**WIP**


&nbsp;

## Installation (optional)

```bash
git clone https://github.com/Renovamen/torchop.git
cd torchop
python setup.py install
```

or

```bash
pip install git+https://github.com/Renovamen/torchop.git --upgrade
```


&nbsp;

## Implemented Networks

- Vanilla Attention

  [Neural Machine Translation by Jointly Learning to Align and Translate.](https://arxiv.org/abs/1409.0473) ICLR 2015.

  [Effective Approaches to Attention-based Neural Machine Translation.](https://arxiv.org/abs/1508.04025) EMNLP 2015.

- Self-Attention, Simplified Self-Attention

  [Attention Is All You Need.](https://arxiv.org/abs/1706.03762) NIPS 2017.

- SAGAN Attention

  [Self-Attention Generative Adversarial Networks.](https://arxiv.org/abs/1805.08318) ICML 2019.

- External Attention

  [Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks.](https://arxiv.org/abs/2105.02358) arXiv 2021.

- Fast Attention (proposed in Fastformer)

  [Fastformer: Additive Attention Can Be All You Need.](https://arxiv.org/abs/2108.09084) arXiv 2021.
